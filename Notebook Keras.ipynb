{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Convolution2D, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from wrappers import wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-429e77cb0674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDeepQNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepQNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutNoFrameskip-v4')\n",
    "n_actions = env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size= 8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size, stride):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = conv2d_size_out(conv2d_size_out(\n",
    "            conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        linear_input = convh * convw * 64\n",
    "        self.fc1 = nn.Linear(linear_input, 512)\n",
    "        self.out = nn.Linear(512, n_actions)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x.float()))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(current_step):\n",
    "    rate = (EPS_END-EPS_START)/MEM_SIZE\n",
    "    eps_threshold = rate * current_step + EPS_START\n",
    "    if eps_threshold < EPS_END:\n",
    "        return EPS_END\n",
    "    return eps_threshold\n",
    "\n",
    "\n",
    "def select_action(state, steps_done, eval=False):\n",
    "    global EPSILON\n",
    "\n",
    "    # This equation is for the decaying epsilon\n",
    "    eps_threshold = get_epsilon(steps_done)\n",
    "\n",
    "    if eval:\n",
    "        eps_threshold = EPS_END\n",
    "\n",
    "    r = np.random.rand()\n",
    "\n",
    "    EPSILON = eps_threshold\n",
    "\n",
    "    # We select an action with an espilon greedy policy \n",
    "    if r > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Return the action with the maximum Q value for the current state\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if memory.current < memory.batch_size:\n",
    "        return 0\n",
    "    optimizer.zero_grad()\n",
    "    states, actions, rewards, next_states, dones = memory.get_minibatch()\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done = torch.tensor(dones).to(device)\n",
    "\n",
    "    state_action_values = policy_net(states_v).gather(\n",
    "        1, actions_v.long().unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = target_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "     \n",
    "    # Compute Huber loss\n",
    "    loss = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step() \n",
    "    \n",
    "    return loss.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
